
The technical task goal was to create a model to predict the name of the taster for a dataset of wine reviews. The data itself was a series of categorical columns, two numerical columns(price, points), and one text column(description). 

I decided to start by looking through the various aspects of the dataframe, to see what the data looked like. This includes drawing different graphs to look at the numerical features of price and points, looking at the text entries of the description column, and deciding which of the various features had the best types of information from which an algorithm could determine differences.  Given that the task for this assignment was to predict which taster gave which review, I found it critical to keep in all of the tasters, even though that did affect performance. Although I looked at the tasters with smaller numbers of reviews, and while I kept them in to prevent overfitting and to keep the task in mind, could see a successful model being built after taking out the tasters who did not have a "significant" number of reviews.

In order to create a model that accurately predicts which taster wrote which review, I decided to make two dataframes from the original dataset upon which to base the models. One approach, was to use the categorical features and create dummy variables of those features with the most "reasonable" number of categories. For example, creating dummy variables for a category with ~120,000 categories when the dataset was only ~130,000 rows large, does not give distinct enough information for an algorithm to detect. And so, I did not make dummy variables for those columns with category numbers that were too large. 

As a second dataframe, I decided to delve into my passion of NLP, and create a dataframe based entirely on the text of the descriptions of the wines, written by the tasters. The natural language processing dataframe was made up entirely of the words that were used in these descriptions, which, when looking at the descriptions, are mostly flowery adjectives describing the various complex flavors of the different wines. This indicates that further natural language processing would be interesting, given that most of the words are adjectives, and different tasters may repeat use of their favorite adjectives.

I found that the big issue with this dataset was the idea of overfitting a model, so I performed a series of tests on the different datasets with different models to determine the best model for the best dataframe. I built a for-loop wherein I could test four different models on one dataset, just to get a general sense of performance. Those loops took anywhere between 20 minutes to 8 hours to complete, dependent on the size of the dataframe, so for the most part, I commented them out, so the run time would be quicker. In the end, the natural language processing dataset performed better than the dataset made up of dummy-variables, however both performed significantly better than the baseline (most common taster). This indicates that there are many ways to move forward depending on what types of information are most important, interpretability, using text analysis or using the various features given in the dataset, including location, variety, etc. 

With regard to different things I would do with more data, more time or more information about what the model will be used for, I have a few ideas. Firstly, I think that making dummy variables worked very well for model performance, but had I known which features were most important to those requesting the model, I could have picked different features or different numbers of columns from which to pull. In addition, I could perform additional natural language processing work, such as addressing the large amounts adjectives by using part of speech analysis, or test the different tones of the different tasters with sentiment analysis. Another issue is interpretability, I could have used more black-box models if I was not concerned with interpretability, however if it was crucial to know which features were the most important, I would have run different types of models to figure out not just the most commonly used, but also the most important features to the predictions.

I added comments throughout the code to explain each step and each decision, but if there are any remaining questions as to why I did something, please reach out via Skype or email. I look forward to hearing from you soon!
